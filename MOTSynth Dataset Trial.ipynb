{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac88b33-4111-4977-a5ba-a1862fc64c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from motsynth import MOTSynth\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472dfbb-db69-434c-831a-e9a9ed54a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/storage/user/brasoand/motsyn2/frames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebff41e-19ec-4236-bf3b-601fe80a2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOTSynth(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe0eb7-2fbd-43c3-ad40-08653f22207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "        data, batch_size=4, shuffle=True, num_workers=args.workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e57e1-8d8e-47da-af09-af118c542faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d865fa1-c97a-4d7c-b2f3-5c6e3a0b82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "suitable_video_list_only_from_top = [2, 10, 19, 28, 33, 34, 42, 43, 50, 52, 59, 60, 62, 68,\n",
    "                                     69, 73, 77, 78, 84, 86, 87, 90, 94, 96, 103, 104, 112, 113,\n",
    "                                     121, 122, 130, 138, 147, 156, 161, 162, 170, 171, 178, 180,\n",
    "                                     187, 188, 196, 197]\n",
    "len(suitable_video_list_only_from_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b8534-5b90-465c-b62c-0a4b7a9ed990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a0857-59a4-489d-bbbd-97867ad2295a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780d39b-5881-4502-987e-5cb0c60fa7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80308ffd-19d2-4698-994a-648d387a1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/storage/user/brasoand/motsyn2/annotations/001.json\") as fp:\n",
    "    annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122e63c-08f4-4a34-80c6-56fa53221568",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = [an.get('segmentation') for an in annotation.get('annotations')]\n",
    "frame_id = 10000\n",
    "# frame_indices = [an.get('image_id') for an in annotation.get('annotations')]\n",
    "frame_indices = np.array([an.get('image_id') for an in annotation.get('annotations')])\n",
    "changed_indices = np.where(frame_indices[:-1] != frame_indices[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f093e01-2232-422a-8eeb-194c2df5d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d4751-0855-4eaf-b146-48ebd4e30020",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_indices[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ec699-34f0-4aed-be2a-749151559fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83398eb-2910-426a-b315-3dff0232a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation.get('annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e2c98-59ed-416f-aa38-afd59e18b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a933b4-2eb1-42e2-ae99-0327a30d3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794305a-d606-439e-8c7f-38e4305230c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        if isinstance(polygons['counts'], list):\n",
    "            rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        \n",
    "        else:\n",
    "            rles = [polygons]\n",
    "\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        #mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        #mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    # if masks:\n",
    "    #     masks = torch.stack(masks, dim=0)\n",
    "    # else:\n",
    "    #     masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c064b49-de77-4512-a32f-62df7e5d6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_bbg(video_id, global_idx):\n",
    "    with open(f\"/storage/user/brasoand/motsyn2/annotations/{video_id}.json\") as fp:\n",
    "        annotation = json.load(fp)\n",
    "    print(\"Annotation File loaded.\")\n",
    "    frame_src = f\"/storage/user/brasoand/motsyn2/frames/{video_id}/rgb/\"\n",
    "    segmentations = [an.get('segmentation') for an in annotation.get('annotations')]\n",
    "    \n",
    "#     frame_indices = [an.get('image_id') for an in annotation.get('annotations')]\n",
    "    frame_indices = np.array([an.get('image_id') for an in annotation.get('annotations')])\n",
    "    changed_indices = np.where(frame_indices[:-1] != frame_indices[1:])[0]\n",
    "    constant = frame_indices[0]\n",
    "    prev_change = 0\n",
    "    total_masks_list = []\n",
    "    black_bgg_img_list = []\n",
    "    for k, idx in enumerate(changed_indices):\n",
    "        frame_id = frame_indices[prev_change] - constant\n",
    "        print(frame_id)\n",
    "        original_img =mpimg.imread(f'/storage/user/brasoand/motsyn2/frames/{video_id}/rgb/{str(frame_id).zfill(4)}.jpg')\n",
    "        f_segmentations = segmentations[prev_change:(idx+1)]\n",
    "        prev_change = idx+1\n",
    "        masks = [convert_coco_poly_to_mask([s], 1920, 1080) for s in f_segmentations]\n",
    "        total_masks = np.sum(masks, 0).squeeze()\n",
    "#         total_masks_list.append(total_masks)\n",
    "\n",
    "        masked_img = np.uint8(original_img * total_masks.reshape((1080,1920,1)))\n",
    "        im = Image.fromarray(masked_img)\n",
    "        im.save(f\"/usr/stud/gueley/Git/SCALOR/static_bc_frames/{str(global_idx).zfill(6)}.jpeg\")\n",
    "        global_idx += 1\n",
    "        \n",
    "#         black_bgg_img_list.append(np.uint8(masked_img))\n",
    "#         print(f\"{k}/{len(changed_indices)} done.\", end='\\r')\n",
    "        \n",
    "    return global_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844250d-9671-46bb-9038-4bbcf398e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_indices = np.array([an.get('image_id') for an in annotation.get('annotations')])\n",
    "# changed_indices = np.where(frame_indices[:-1] != frame_indices[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716f473-73e5-4bb8-af2f-ba5dd4e65fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"2\".zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97ed40-b89b-4f84-8856-5765b55713e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_bgg_img_list, total_masks_list = exclude_bbg(\"001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2629f5f-a119-4ac1-b42e-1984f6175bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(np.uint8(black_bgg_img_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead15cd-5022-4338-972e-3761a11deaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in changed_indices:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d42f58-3f2e-49bd-9ed5-3dcb838e38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices[changed_indices[0]+1:changed_indices[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aafdf4-e87c-4078-bd0d-a781162a28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_indices[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2be5d-32e8-4d37-9e71-25decd1eb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frame_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767e842-0f4d-46f4-8fb8-85aa9abb86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.get('annotations')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289448be-31de-4c29-8e8c-29f7998e6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob(\"static_bc_frames_2/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97372a0-25f1-46ad-8565-341872c5e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"static_bc_frames_2\"\n",
    "frame_dirs = sorted([os.path.join(data_dir, a) for a in os.listdir(data_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd645816-89ed-41da-b788-d7478be45ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a40b8-8883-427a-83c9-04b61a334895",
   "metadata": {},
   "outputs": [],
   "source": [
    "suitable_video_list_only_from_top = [2, 10, 19, 28, 33, 34, 42, 43, 50, 52, 59, 60, 62, 68,\n",
    "                                     69, 73, 77, 78, 84, 86, 87, 90, 94, 96, 103, 104, 112, 113,\n",
    "                                     121, 122, 130, 138, 147, 156, 161, 162, 170, 171, 178, 180,\n",
    "                                     187, 188, 196, 197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7346715-7a6f-4e13-acaa-9c16a030aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(suitable_video_list_only_from_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009e35f-b6fa-4f2e-9582-c90dce57d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from common import *\n",
    "from PIL import Image, ImageFile\n",
    "import glob\n",
    "import itertools\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import time\n",
    "class MOTSynthBlackBG(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.skip_freq = 5\n",
    "        self.phase_train = train\n",
    "        \n",
    "        self.frame_dirs = sorted([os.path.join(data_dir, a) for a in os.listdir(data_dir)])\n",
    "        \n",
    "        \n",
    "        if len(self.frame_dirs) != 79200:\n",
    "            raise Exception(\"Something is not correct.\")\n",
    "        \n",
    "#         if self.phase_train:\n",
    "#             self.frame_dirs = self.frame_dirs[:-(len(self.frame_dirs) // 10)]\n",
    "#         else:\n",
    "#             self.frame_dirs = self.frame_dirs[-(len(self.frame_dirs) // 10):]\n",
    "            \n",
    "        print(\"Frame Dirs constructed.\", flush=True)\n",
    "        self.video_id = 0\n",
    "        self.idx = 0\n",
    "        print(\"Dataset initialized.\", flush=True)\n",
    "        \n",
    "    def get_frames(self, index, seq_len=10, skip_freq=5):\n",
    "        b = np.random.randint(0,5,(1))[0]\n",
    "        starting_idx = index * (seq_len * skip_freq) + np.random.randint(0,5,(1))[0]\n",
    "        return np.arange(starting_idx, starting_idx + (seq_len*skip_freq), 5 )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index_list = self.get_frames(index, seq_len, self.skip_freq)\n",
    "        image_list = []\n",
    "        k = int(np.random.rand(1)[0]*840)\n",
    "        print(index_list)\n",
    "        for idx in index_list:\n",
    "            f_n = self.frame_dirs[idx]\n",
    "            im = Image.open(f_n)\n",
    "                \n",
    "            im = im.crop(box=(k, 0, 1920 - (840-k) ,1080))\n",
    "            im_array = np.array(im)\n",
    "            \n",
    "            im = im.resize((img_h, img_w), resample=Image.BILINEAR)\n",
    "            im_tensor = torch.from_numpy(np.array(im) / 255).permute(2, 0, 1)\n",
    "            image_list.append(im_tensor)\n",
    "\n",
    "\n",
    "        img = torch.stack(image_list, dim=0)\n",
    "\n",
    "        return img.float(), torch.zeros(1)\n",
    "    def __len__(self):\n",
    "        return int(len(self.frame_dirs) / (seq_len * self.skip_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78f627-fbd9-463a-b6a2-f1b3e46c313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MOTSynthBlackBG(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cb2df-826a-440e-8981-4fc96cc966c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.frame_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e56904-39c1-4de0-83f1-c2c09cad2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "        dataset, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0d857-7f23-4300-9b98-780dfcda7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = dataset.__getitem__(1583)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf80d7-c77c-41a5-b750-d924adf452e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11576ea9-9a2c-4daf-a5b5-3db822e34b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c385-9cc8-492e-b154-6073b809b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "79200/50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4afe5b-dfdf-4165-a704-b9db8a6e5ffd",
   "metadata": {},
   "source": [
    "#### Single Video Batch DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451185c-a5af-48f3-a613-25b01a6a7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from common import *\n",
    "from PIL import Image, ImageFile\n",
    "import glob\n",
    "import itertools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "top_down_list = [19, 28, 33, 42, 59, 60, 68, 73, 86,\n",
    "                 94, 103, 112, 121, 130, 147, 161, \n",
    "                 170, 178, 187]\n",
    "top_down_list = [str(a).zfill(3) for a in top_down_list]\n",
    "\n",
    "class MOTSynthV2(Dataset):\n",
    "    def __init__(self, data_dir, train=False):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.skip_freq = 5\n",
    "        self.phase_train = train\n",
    "        self.current_video = None\n",
    "        #         self.video_dirs = [os.path.join(data_dir, video_dir) for video_dir in os.listdir(data_dir)]\n",
    "        \n",
    "        print(f\"Dataset Length: {len(top_down_list)}\")\n",
    "        self.video_dirs = [\n",
    "            os.path.join(data_dir, video_dir) for video_dir in top_down_list\n",
    "        ]\n",
    "\n",
    "        random.shuffle(self.video_dirs)\n",
    "        \n",
    "        # if self.phase_train:\n",
    "        #     self.video_dirs = self.video_dirs[: -(len(self.video_dirs) // 10)]\n",
    "        # else:\n",
    "        #     self.video_dirs = self.video_dirs[-(len(self.video_dirs) // 10) :]\n",
    "        print(f\"DataSet Used: MOTSynth\", flush=True)\n",
    "        print(f\"Suitable Videos: {self.video_dirs}\", flush=True)\n",
    "        print(f\"Suitable Videos Length: {len(self.video_dirs)}\", flush=True)\n",
    "        print(\"Video Dirs constructed.\", flush=True)\n",
    "        self.frame_dirs = [\n",
    "            sorted(glob.glob(video_dir + \"/rgb/*\")) for video_dir in self.video_dirs\n",
    "        ]\n",
    "        self.frame_dirs = sorted(list(itertools.chain(*self.frame_dirs)))\n",
    "        print(\"Frame Dirs constructed.\", flush=True)\n",
    "        #         self.frame_dirs = []\n",
    "        self.video_id = 0\n",
    "        self.idx = 0\n",
    "        print(\"Dataset initialized.\", flush=True)\n",
    "\n",
    "    def get_frames(self, index, seq_len=10, skip_freq=5):\n",
    "        b = np.random.randint(0, 5, (1))[0]\n",
    "        starting_idx = index * (seq_len * skip_freq) + np.random.randint(0, 5, (1))[0]\n",
    "        return np.arange(starting_idx, starting_idx + (seq_len * skip_freq), 5)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #         print(index)\n",
    "        current_video_frame_dirs = [f for f in self.frame_dirs if f\"frames/{self.current_video}\" in f]\n",
    "#         print((current_video_frame_dirs[0:5]))\n",
    "        real_index = index % 36\n",
    "        index_list = self.get_frames(real_index, seq_len, self.skip_freq)\n",
    "        image_list = []\n",
    "        #         print(\"-------------------------\", flush=True)\n",
    "        k = int(np.random.rand(1)[0] * 840)\n",
    "        for idx in index_list:\n",
    "#             f_n = self.frame_dirs[idx]\n",
    "            f_n = current_video_frame_dirs[idx]\n",
    "            im = Image.open(f_n)\n",
    "\n",
    "            im = im.crop(box=(k, 0, 1920 - (840 - k), 1080))\n",
    "            im_array = np.array(im)\n",
    "\n",
    "            im = im.resize((img_h, img_w), resample=Image.BILINEAR)\n",
    "            im_tensor = torch.from_numpy(np.array(im) / 255).permute(2, 0, 1)\n",
    "            image_list.append(im_tensor)\n",
    "\n",
    "\n",
    "        img = torch.stack(image_list, dim=0)\n",
    "        print(index)\n",
    "        return img.float(), self.current_video\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.frame_dirs) / (seq_len * self.skip_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a0251-d720-4b2d-96c9-3c358d6a39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/storage/user/brasoand/motsyn2/frames/\"\n",
    "data = MOTSynthV2(data_dir)\n",
    "data.current_video = random.choice(top_down_list)\n",
    "train_loader = DataLoader(\n",
    "        data, batch_size=1, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990306d-4940-4b42-a1f9-e876b69d0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.current_video = random.choice(top_down_list)\n",
    "train_loader.dataset.current_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67379-d419-4607-9a70-3b43b8820bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= data.__getitem__(4)\n",
    "# data.current_video = random.choice(top_down_list)\n",
    "# a = data.__getitem__(4)\n",
    "# data.current_video = random.choice(top_down_list)\n",
    "# a = data.__getitem__(4)\n",
    "data.current_video = random.choice(top_down_list)\n",
    "generator = iter(train_loader)\n",
    "sample, counting_gt = next(generator)\n",
    "print(counting_gt)\n",
    "\n",
    "# generator._dataset.current_video = random.choice(top_down_list)\n",
    "data.current_video = random.choice(top_down_list)\n",
    "\n",
    "sample, counting_gt = next(generator)\n",
    "print(counting_gt)\n",
    "# generator._dataset.current_video = random.choice(top_down_list)\n",
    "data.current_video = random.choice(top_down_list)\n",
    "\n",
    "sample, counting_gt = next(generator)\n",
    "print(counting_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b969cd-3ddf-45c6-88e7-079da6228c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.current_video = random.choice(top_down_list)\n",
    "\n",
    "sample, counting_gt = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e8f6d-8ef1-40ca-9033-b937f6d680a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d55b83-717b-4d56-96b1-b1f1e21e5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.current_video = random.choice(top_down_list)\n",
    "sample, counting_gt = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397a3d4-baf2-412f-b2de-8dc0f71bdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc0054-3a2c-4148-8cdd-b1e6a3360025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.current_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16986a3f-120c-49c8-878e-5ba342bc5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(top_down_list, len(top_down_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70f2f2-bb16-41bd-b80b-a6bd5dd1762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_down_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e906d-b0b0-43f4-b00f-049e1f190647",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.current_video = random.choice(top_down_list)\n",
    "sample, counting_gt = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8858f71-1c0b-4019-a373-cb723036bbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61da4f-9d3a-4909-aaf6-54d18083faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_id_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d3b04f-97c3-4b3e-9c0e-58e9a446eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from common import *\n",
    "from PIL import Image, ImageFile\n",
    "import glob\n",
    "import itertools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e031162a-753d-48d6-ad55-fd05de9499f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_embs = {str(k):torch.randn(10) for k in top_down_list}\n",
    "class MOTSynth(Dataset):\n",
    "    def __init__(self, data_dir, train=False):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.skip_freq = 5\n",
    "        self.phase_train = train\n",
    "        #         self.video_dirs = [os.path.join(data_dir, video_dir) for video_dir in os.listdir(data_dir)]\n",
    "\n",
    "        self.video_dirs = [\n",
    "            os.path.join(data_dir, video_dir) for video_dir in top_down_list\n",
    "        ]\n",
    "\n",
    "        random.shuffle(self.video_dirs)\n",
    "        \n",
    "        # if self.phase_train:\n",
    "        #     self.video_dirs = self.video_dirs[: -(len(self.video_dirs) // 10)]\n",
    "        # else:\n",
    "        #     self.video_dirs = self.video_dirs[-(len(self.video_dirs) // 10) :]\n",
    "        print(f\"DataSet Used: MOTSynth\", flush=True)\n",
    "        print(f\"Suitable Videos: {self.video_dirs}\", flush=True)\n",
    "        print(f\"Suitable Videos Length: {len(self.video_dirs)}\", flush=True)\n",
    "        print(\"Video Dirs constructed.\", flush=True)\n",
    "        self.frame_dirs = [\n",
    "            sorted(glob.glob(video_dir + \"/rgb/*\")) for video_dir in self.video_dirs\n",
    "        ]\n",
    "        self.frame_dirs = sorted(list(itertools.chain(*self.frame_dirs)))\n",
    "        print(\"Frame Dirs constructed.\", flush=True)\n",
    "        #         self.frame_dirs = []\n",
    "        self.video_id = 0\n",
    "        self.idx = 0\n",
    "        print(\"Dataset initialized.\", flush=True)\n",
    "\n",
    "    def get_frames(self, index, seq_len=10, skip_freq=5):\n",
    "        b = np.random.randint(0, 5, (1))[0]\n",
    "        starting_idx = index * (seq_len * skip_freq) + np.random.randint(0, 5, (1))[0]\n",
    "        return np.arange(starting_idx, starting_idx + (seq_len * skip_freq), 5)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #         print(index)\n",
    "        index_list = self.get_frames(index, seq_len, self.skip_freq)\n",
    "        image_list = []\n",
    "        #         print(\"-------------------------\", flush=True)\n",
    "        k = int(np.random.rand(1)[0] * 840)\n",
    "#         k = 0\n",
    "        for idx in index_list:\n",
    "            f_n = self.frame_dirs[idx]\n",
    "            #             print(f_n, flush=True)\n",
    "            im = Image.open(f_n)\n",
    "\n",
    "            im = im.crop(box=(k, 0, 1920 - (840 - k), 1080))\n",
    "            #             print(im.size)\n",
    "            im_array = np.array(im)\n",
    "\n",
    "            #             im = im.crop(box=(left_edge, upper_edge, left_edge + self.args.train_station_cropping_origin,\n",
    "            #                               upper_edge + self.args.train_station_cropping_origin))\n",
    "            im = im.resize((img_h, img_w), resample=Image.BILINEAR)\n",
    "            im_tensor = torch.from_numpy(np.array(im) / 255).permute(2, 0, 1)\n",
    "            image_list.append(im_tensor)\n",
    "        #         print(\"-------------------------\", flush=True)\n",
    "\n",
    "        current_video_id = f_n.split(\"/\")[-3]\n",
    "        print(current_video_id)\n",
    "        img = torch.stack(image_list, dim=0)\n",
    "        #         print(img.shape)\n",
    "        #         self.idx +=1\n",
    "        #         print(f\"Status: {self.idx}/{int(len(self.frame_dirs) / seq_len)} done.\", end='\\r', flush=True)\n",
    "        return img.float(), video_id_embs[current_video_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.frame_dirs) / (seq_len * self.skip_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0b4b2-9f25-4a45-be5d-7eb1dcf94d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/storage/user/brasoand/motsyn2/frames/\"\n",
    "data = MOTSynth(data_dir)\n",
    "data.current_video = random.choice(top_down_list)\n",
    "train_loader = DataLoader(\n",
    "        data, batch_size=10, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc163d6-c5ad-4e8d-904c-8def555f346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iter(train_loader)\n",
    "sample, counting_gt = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e048cec-b67c-4414-929c-5aa42a71e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a30308-ea21-4b97-b9bd-e4f281365d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_down_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cd41-933f-4c7c-ae0c-b4be85e92ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_gt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63561cd4-f32b-4b08-9da1-83f1d92a63be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'028': tensor([0.5698, 0.4523, 0.0439, 0.2748, 0.6137, 0.7246, 0.6272, 0.7679, 0.0314,\n",
       "         0.7192]),\n",
       " '033': tensor([0.3194, 0.7520, 0.0468, 0.1089, 0.3940, 0.0794, 0.8959, 0.4173, 0.8779,\n",
       "         0.1524]),\n",
       " '042': tensor([0.7498, 0.8630, 0.4793, 0.3743, 0.9638, 0.0117, 0.2004, 0.5346, 0.1427,\n",
       "         0.7409]),\n",
       " '059': tensor([0.4577, 0.9213, 0.9728, 0.2543, 0.1908, 0.1222, 0.6117, 0.9399, 0.3682,\n",
       "         0.0634]),\n",
       " '060': tensor([0.0416, 0.6504, 0.6510, 0.1798, 0.1896, 0.4336, 0.1933, 0.5292, 0.7736,\n",
       "         0.6729]),\n",
       " '068': tensor([0.3736, 0.4585, 0.8045, 0.1315, 0.6613, 0.8539, 0.3696, 0.3114, 0.0410,\n",
       "         0.8416]),\n",
       " '073': tensor([0.0547, 0.0443, 0.4126, 0.3588, 0.4887, 0.5848, 0.3670, 0.9690, 0.9729,\n",
       "         0.0097]),\n",
       " '086': tensor([0.0956, 0.6294, 0.2731, 0.4732, 0.8368, 0.2797, 0.9791, 0.6993, 0.5390,\n",
       "         0.4438])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{str(k):torch.rand(10) for k in top_down_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d21c74-a616-4635-940f-cd91b0ba15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b606812-079c-4108-8fe5-e977aa5ec2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329ef899-17c3-4623-b643-8aa5dce73b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'028': tensor([-0.9431,  0.3764, -0.1398, -0.6531,  0.0526, -0.3063, -0.5410, -1.9295,\n",
       "          0.1432, -2.3675]),\n",
       " '033': tensor([-0.0989,  0.6890,  1.2964,  1.3719,  1.8907,  0.0976, -0.6488, -1.6996,\n",
       "         -0.3500,  0.9189]),\n",
       " '042': tensor([ 0.6292, -0.7657, -1.6421, -0.5241, -1.0109,  0.2165, -0.7254,  1.7617,\n",
       "          0.3518,  0.9029]),\n",
       " '059': tensor([-0.8495, -0.7542,  0.8164,  0.6392,  2.3996,  0.7730, -0.3945, -1.1212,\n",
       "          0.6336, -0.9527]),\n",
       " '060': tensor([ 0.7131,  0.4176, -0.6837,  1.5498, -1.0839, -0.6990,  0.3572, -0.7721,\n",
       "          0.9399,  0.6562]),\n",
       " '068': tensor([-0.0179, -1.2116,  0.6126,  0.4204,  0.9386, -0.8316, -0.5078, -0.2547,\n",
       "          0.9024, -0.6797]),\n",
       " '073': tensor([ 0.5031, -0.6749, -0.7594, -0.2348,  0.1629, -0.4459,  0.9873,  0.3375,\n",
       "         -1.0550, -0.2310]),\n",
       " '086': tensor([ 0.4655,  0.6293,  1.0465, -1.1338, -1.1782,  0.2136, -0.8174, -0.1605,\n",
       "         -0.7459, -0.4709])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'028': torch.tensor([0.5698, 0.4523, 0.0439, 0.2748, 0.6137, 0.7246, 0.6272, 0.7679, 0.0314,\n",
    "         0.7192]),\n",
    " '033': torch.tensor([0.3194, 0.7520, 0.0468, 0.1089, 0.3940, 0.0794, 0.8959, 0.4173, 0.8779,\n",
    "         0.1524]),\n",
    " '042': torch.tensor([0.7498, 0.8630, 0.4793, 0.3743, 0.9638, 0.0117, 0.2004, 0.5346, 0.1427,\n",
    "         0.7409]),\n",
    " '059': torch.tensor([0.4577, 0.9213, 0.9728, 0.2543, 0.1908, 0.1222, 0.6117, 0.9399, 0.3682,\n",
    "         0.0634]),\n",
    " '060': torch.tensor([0.0416, 0.6504, 0.6510, 0.1798, 0.1896, 0.4336, 0.1933, 0.5292, 0.7736,\n",
    "         0.6729]),\n",
    " '068': torch.tensor([0.3736, 0.4585, 0.8045, 0.1315, 0.6613, 0.8539, 0.3696, 0.3114, 0.0410,\n",
    "         0.8416]),\n",
    " '073': torch.tensor([0.0547, 0.0443, 0.4126, 0.3588, 0.4887, 0.5848, 0.3670, 0.9690, 0.9729,\n",
    "         0.0097]),\n",
    " '086': torch.tensor([0.0956, 0.6294, 0.2731, 0.4732, 0.8368, 0.2797, 0.9791, 0.6993, 0.5390,\n",
    "         0.4438])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4647a56-fe04-4b1b-8eb8-3962041cee14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
